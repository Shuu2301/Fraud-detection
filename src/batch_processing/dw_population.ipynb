{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "49a35920",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql import SparkSession\n",
    "from pyspark.sql.functions import *\n",
    "from pyspark.sql.types import *\n",
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "610df923",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Configuration variables\n",
    "MINIO_ENDPOINT = 'http://localhost:9900'\n",
    "MINIO_ACCESS_KEY = 'minioadmin'\n",
    "MINIO_SECRET_KEY = 'minioadmin123'\n",
    "MYSQL_HOST = 'localhost'\n",
    "MYSQL_PORT = '30306'\n",
    "MYSQL_DATABASE = 'finance_dw'\n",
    "MYSQL_USER = 'root'\n",
    "MYSQL_PASSWORD = 'root12345'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "211b202d",
   "metadata": {},
   "outputs": [
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mKeyboardInterrupt\u001b[39m                         Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[6]\u001b[39m\u001b[32m, line 30\u001b[39m\n\u001b[32m      1\u001b[39m builder = (\n\u001b[32m      2\u001b[39m     SparkSession.builder.appName(\u001b[33m\"\u001b[39m\u001b[33mDataWarehouse-ETL\u001b[39m\u001b[33m\"\u001b[39m)\n\u001b[32m      3\u001b[39m     \u001b[38;5;66;03m# Memory configurations for ETL processing\u001b[39;00m\n\u001b[32m   (...)\u001b[39m\u001b[32m     27\u001b[39m     .config(\u001b[33m\"\u001b[39m\u001b[33mspark.hadoop.fs.s3a.retry.limit\u001b[39m\u001b[33m\"\u001b[39m, \u001b[33m\"\u001b[39m\u001b[33m3\u001b[39m\u001b[33m\"\u001b[39m)\n\u001b[32m     28\u001b[39m )\n\u001b[32m---> \u001b[39m\u001b[32m30\u001b[39m spark = \u001b[43mbuilder\u001b[49m\u001b[43m.\u001b[49m\u001b[43mgetOrCreate\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\admin\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\pyspark\\sql\\session.py:556\u001b[39m, in \u001b[36mSparkSession.Builder.getOrCreate\u001b[39m\u001b[34m(self)\u001b[39m\n\u001b[32m    554\u001b[39m     sparkConf.set(key, value)\n\u001b[32m    555\u001b[39m \u001b[38;5;66;03m# This SparkContext may be an existing one.\u001b[39;00m\n\u001b[32m--> \u001b[39m\u001b[32m556\u001b[39m sc = \u001b[43mSparkContext\u001b[49m\u001b[43m.\u001b[49m\u001b[43mgetOrCreate\u001b[49m\u001b[43m(\u001b[49m\u001b[43msparkConf\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    557\u001b[39m \u001b[38;5;66;03m# Do not update `SparkConf` for existing `SparkContext`, as it's shared\u001b[39;00m\n\u001b[32m    558\u001b[39m \u001b[38;5;66;03m# by all sessions.\u001b[39;00m\n\u001b[32m    559\u001b[39m session = SparkSession(sc, options=\u001b[38;5;28mself\u001b[39m._options)\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\admin\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\pyspark\\core\\context.py:523\u001b[39m, in \u001b[36mSparkContext.getOrCreate\u001b[39m\u001b[34m(cls, conf)\u001b[39m\n\u001b[32m    521\u001b[39m \u001b[38;5;28;01mwith\u001b[39;00m SparkContext._lock:\n\u001b[32m    522\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m SparkContext._active_spark_context \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[32m--> \u001b[39m\u001b[32m523\u001b[39m         \u001b[43mSparkContext\u001b[49m\u001b[43m(\u001b[49m\u001b[43mconf\u001b[49m\u001b[43m=\u001b[49m\u001b[43mconf\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;129;43;01mor\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mSparkConf\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    524\u001b[39m     \u001b[38;5;28;01massert\u001b[39;00m SparkContext._active_spark_context \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[32m    525\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m SparkContext._active_spark_context\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\admin\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\pyspark\\core\\context.py:205\u001b[39m, in \u001b[36mSparkContext.__init__\u001b[39m\u001b[34m(self, master, appName, sparkHome, pyFiles, environment, batchSize, serializer, conf, gateway, jsc, profiler_cls, udf_profiler_cls, memory_profiler_cls)\u001b[39m\n\u001b[32m    199\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m gateway \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;129;01mand\u001b[39;00m gateway.gateway_parameters.auth_token \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[32m    200\u001b[39m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\n\u001b[32m    201\u001b[39m         \u001b[33m\"\u001b[39m\u001b[33mYou are trying to pass an insecure Py4j gateway to Spark. This\u001b[39m\u001b[33m\"\u001b[39m\n\u001b[32m    202\u001b[39m         \u001b[33m\"\u001b[39m\u001b[33m is not allowed as it is a security risk.\u001b[39m\u001b[33m\"\u001b[39m\n\u001b[32m    203\u001b[39m     )\n\u001b[32m--> \u001b[39m\u001b[32m205\u001b[39m \u001b[43mSparkContext\u001b[49m\u001b[43m.\u001b[49m\u001b[43m_ensure_initialized\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mgateway\u001b[49m\u001b[43m=\u001b[49m\u001b[43mgateway\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mconf\u001b[49m\u001b[43m=\u001b[49m\u001b[43mconf\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    206\u001b[39m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[32m    207\u001b[39m     \u001b[38;5;28mself\u001b[39m._do_init(\n\u001b[32m    208\u001b[39m         master,\n\u001b[32m    209\u001b[39m         appName,\n\u001b[32m   (...)\u001b[39m\u001b[32m    219\u001b[39m         memory_profiler_cls,\n\u001b[32m    220\u001b[39m     )\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\admin\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\pyspark\\core\\context.py:444\u001b[39m, in \u001b[36mSparkContext._ensure_initialized\u001b[39m\u001b[34m(cls, instance, gateway, conf)\u001b[39m\n\u001b[32m    442\u001b[39m \u001b[38;5;28;01mwith\u001b[39;00m SparkContext._lock:\n\u001b[32m    443\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m SparkContext._gateway:\n\u001b[32m--> \u001b[39m\u001b[32m444\u001b[39m         SparkContext._gateway = gateway \u001b[38;5;129;01mor\u001b[39;00m \u001b[43mlaunch_gateway\u001b[49m\u001b[43m(\u001b[49m\u001b[43mconf\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    445\u001b[39m         SparkContext._jvm = SparkContext._gateway.jvm\n\u001b[32m    447\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m instance:\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\admin\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\pyspark\\java_gateway.py:108\u001b[39m, in \u001b[36mlaunch_gateway\u001b[39m\u001b[34m(conf, popen_kwargs)\u001b[39m\n\u001b[32m    106\u001b[39m \u001b[38;5;66;03m# Wait for the file to appear, or for the process to exit, whichever happens first.\u001b[39;00m\n\u001b[32m    107\u001b[39m \u001b[38;5;28;01mwhile\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m proc.poll() \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m os.path.isfile(conn_info_file):\n\u001b[32m--> \u001b[39m\u001b[32m108\u001b[39m     \u001b[43mtime\u001b[49m\u001b[43m.\u001b[49m\u001b[43msleep\u001b[49m\u001b[43m(\u001b[49m\u001b[32;43m0.1\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[32m    110\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m os.path.isfile(conn_info_file):\n\u001b[32m    111\u001b[39m     \u001b[38;5;28;01mraise\u001b[39;00m PySparkRuntimeError(\n\u001b[32m    112\u001b[39m         errorClass=\u001b[33m\"\u001b[39m\u001b[33mJAVA_GATEWAY_EXITED\u001b[39m\u001b[33m\"\u001b[39m,\n\u001b[32m    113\u001b[39m         messageParameters={},\n\u001b[32m    114\u001b[39m     )\n",
      "\u001b[31mKeyboardInterrupt\u001b[39m: "
     ]
    }
   ],
   "source": [
    "builder = (\n",
    "    SparkSession.builder.appName(\"DataWarehouse-ETL\")\n",
    "    # Memory configurations for ETL processing\n",
    "    .config(\"spark.driver.memory\", \"4g\")\n",
    "    .config(\"spark.executor.memory\", \"4g\")\n",
    "    .config(\"spark.driver.maxResultSize\", \"2g\")\n",
    "    .config(\"spark.sql.adaptive.enabled\", \"true\")\n",
    "    .config(\"spark.sql.adaptive.coalescePartitions.enabled\", \"true\")\n",
    "    # Jars for Delta Lake, S3, and MySQL\n",
    "    .config(\"spark.jars.packages\", \"org.apache.hadoop:hadoop-aws:3.4.1,com.amazonaws:aws-java-sdk-bundle:1.12.262,\" \\\n",
    "    \"io.delta:delta-spark_2.13:4.0.0,\" \\\n",
    "    \"com.mysql:mysql-connector-j:8.0.33\")\n",
    "    # Delta Lake\n",
    "    .config(\"spark.sql.extensions\", \"io.delta.sql.DeltaSparkSessionExtension\")\n",
    "    .config(\"spark.sql.catalog.spark_catalog\", \"org.apache.spark.sql.delta.catalog.DeltaCatalog\")\n",
    "    # MinIO (S3A) - Source\n",
    "    .config(\"spark.hadoop.fs.s3a.endpoint\", MINIO_ENDPOINT)\n",
    "    .config(\"spark.hadoop.fs.s3a.access.key\", MINIO_ACCESS_KEY)\n",
    "    .config(\"spark.hadoop.fs.s3a.secret.key\", MINIO_SECRET_KEY)\n",
    "    .config(\"spark.hadoop.fs.s3a.path.style.access\", \"true\")\n",
    "    .config(\"spark.hadoop.fs.s3a.impl\", \"org.apache.hadoop.fs.s3a.S3AFileSystem\")\n",
    "    # S3A performance configs\n",
    "    .config(\"spark.hadoop.fs.s3a.connection.timeout\", \"60000\")\n",
    "    .config(\"spark.hadoop.fs.s3a.connection.request.timeout\", \"60000\")\n",
    "    .config(\"spark.hadoop.fs.s3a.connection.ssl.enabled\", \"false\")\n",
    "    .config(\"spark.hadoop.fs.s3a.attempts.maximum\", \"3\")\n",
    "    .config(\"spark.hadoop.fs.s3a.retry.limit\", \"3\")\n",
    ")\n",
    "\n",
    "spark = builder.getOrCreate()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1e5bfaa4",
   "metadata": {},
   "outputs": [],
   "source": [
    "dw_mysql_url = f\"jdbc:mysql://{MYSQL_HOST}:{MYSQL_PORT}/{MYSQL_DATABASE}\"\n",
    "dw_mysql_properties = {\n",
    "    \"user\": MYSQL_USER,\n",
    "    \"password\": MYSQL_PASSWORD,\n",
    "    \"driver\": \"com.mysql.cj.jdbc.Driver\"\n",
    "}\n",
    "\n",
    "# tables_config = {\n",
    "#         'users': {\n",
    "#             'path': 's3a://warehouse/dim_user/',\n",
    "#             'partitions': None\n",
    "#         },\n",
    "#         'mcc_codes': {\n",
    "#             'path': 's3a://warehouse/dim_mcc/',\n",
    "#             'partitions': None\n",
    "#         },\n",
    "#         'cards': {\n",
    "#             'path': 's3a://warehouse/dim_card/',\n",
    "#             'partitions': ['card_brand']  # Partition by brand for better queries\n",
    "#         },\n",
    "#         'transactions': {\n",
    "#             'path': 's3a://warehouse/fact_transactions/',\n",
    "#             'partitions': ['year', 'month']  # Partition by date for performance\n",
    "#         },\n",
    "#         'fraud_labels': {\n",
    "#             'path': 's3a://warehouse/fraud_labels/',\n",
    "#             'partitions': None\n",
    "#         }\n",
    "#     }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "64ff79c5",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "25/09/26 16:32:10 WARN MetricsConfig: Cannot locate configuration: tried hadoop-metrics2-s3a-file-system.properties,hadoop-metrics2.properties\n",
      "SLF4J: Failed to load class \"org.slf4j.impl.StaticLoggerBinder\".\n",
      "SLF4J: Defaulting to no-operation (NOP) logger implementation\n",
      "SLF4J: See http://www.slf4j.org/codes.html#StaticLoggerBinder for further details.\n",
      "25/09/26 16:32:13 WARN SparkStringUtils: Truncated the string representation of a plan since it was too large. This behavior can be adjusted by setting 'spark.sql.debug.maxToStringFields'.\n",
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "users: 2000 rows loaded\n",
      "mcc_codes: 109 rows loaded\n",
      "cards: 6146 rows loaded\n",
      "transactions: 13305915 rows loaded\n",
      "fraud_labels: 8914963 rows loaded\n"
     ]
    }
   ],
   "source": [
    "users_df = spark.read.format(\"delta\").load(\"s3a://rootdb/users/\")\n",
    "mcc_codes_df = spark.read.format(\"delta\").load(\"s3a://rootdb/mcc_codes/\")\n",
    "cards_df = spark.read.format(\"delta\").load(\"s3a://rootdb/cards/\")\n",
    "transactions_df = spark.read.format(\"delta\").load(\"s3a://rootdb/transactions/\")\n",
    "fraud_labels_df = spark.read.format(\"delta\").load(\"s3a://rootdb/fraud_labels/\")\n",
    "\n",
    "print(f\"users: {users_df.count()} rows loaded\")\n",
    "print(f\"mcc_codes: {mcc_codes_df.count()} rows loaded\")\n",
    "print(f\"cards: {cards_df.count()} rows loaded\")\n",
    "print(f\"transactions: {transactions_df.count()} rows loaded\")\n",
    "print(f\"fraud_labels: {fraud_labels_df.count()} rows loaded\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "4a9e291b",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "25/09/26 16:32:23 WARN S3ABlockOutputStream: Application invoked the Syncable API against stream writing to dim_mcc/part-00000-77d94559-fed9-45a5-9271-5340d83ae67d-c000.snappy.parquet. This is Unsupported\n"
     ]
    }
   ],
   "source": [
    "# POPULATE dim_mcc\n",
    "dim_mcc_df = mcc_codes_df.select(\n",
    "    col(\"mcc\"),\n",
    "    col(\"merchant_type\")\n",
    ")\n",
    "\n",
    "# dim_mcc_df.write \\\n",
    "#     .format(\"jdbc\") \\\n",
    "#     .option(\"url\", dw_mysql_url) \\\n",
    "#     .option(\"dbtable\", \"dim_mcc\") \\\n",
    "#     .options(**dw_mysql_properties) \\\n",
    "#     .mode(\"append\") \\\n",
    "#     .save()\n",
    "\n",
    "dim_mcc_df.write.format(\"delta\").mode(\"overwrite\").save('s3a://warehouse/dim_mcc/')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "fac5e8a9",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "# POPULATE dim_date\n",
    "\n",
    "min_date = transactions_df.select(min(\"trans_date\")).collect()[0][0]\n",
    "max_date = transactions_df.select(max(\"trans_date\")).collect()[0][0]\n",
    "\n",
    "date_range_df = spark.sql(f\"\"\"\n",
    "    SELECT sequence(to_date('{min_date}'), to_date('{max_date}'), interval 1 day) as date_array\n",
    "\"\"\").select(explode(col(\"date_array\")).alias(\"full_date\"))\n",
    "\n",
    "dim_date_df = date_range_df.select(\n",
    "    # Create surrogate key as YYYYMMDD integer\n",
    "    date_format(col(\"full_date\"), \"yyyyMMdd\").cast(\"int\").alias(\"date_sk\"),\n",
    "    col(\"full_date\"),\n",
    "    dayofmonth(col(\"full_date\")).alias(\"day\"),\n",
    "    weekofyear(col(\"full_date\")).alias(\"week\"),\n",
    "    month(col(\"full_date\")).alias(\"month\"),\n",
    "    quarter(col(\"full_date\")).alias(\"quarter\"),\n",
    "    year(col(\"full_date\")).alias(\"year\"),\n",
    "    dayofweek(col(\"full_date\")).alias(\"day_of_week\"),\n",
    "    when(dayofweek(col(\"full_date\")).isin([1, 7]), 1).otherwise(0).alias(\"is_weekend\")\n",
    ")\n",
    "\n",
    "# dim_date_df.write \\\n",
    "#     .format(\"jdbc\") \\\n",
    "#     .option(\"url\", dw_mysql_url) \\\n",
    "#     .option(\"dbtable\", \"dim_date\") \\\n",
    "#     .options(**dw_mysql_properties) \\\n",
    "#     .mode(\"append\") \\\n",
    "#     .save()\n",
    "\n",
    "dim_date_df.write.format(\"delta\").mode(\"overwrite\").save('s3a://warehouse/dim_date/')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "9719ae3b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 3. POPULATE dim_users\n",
    "\n",
    "# Create customer dimension with surrogate keys\n",
    "dim_user_df = users_df.select(\n",
    "    col(\"client_id\"),\n",
    "    col(\"client_id\").alias(\"user_sk\"),  # Using natural key as surrogate for now\n",
    "    col(\"birth_year\"),\n",
    "    col(\"gender\"),\n",
    "    col(\"yearly_income\"),\n",
    "    col(\"total_debt\"),\n",
    "    col(\"credit_score\"),\n",
    "    col(\"num_credit_cards\"),\n",
    "    col(\"latitude\"),\n",
    "    col(\"longitude\"),\n",
    "    # Create user segment based on credit score\n",
    "    when(col(\"credit_score\") >= 750, \"Premium\")\n",
    "    .when(col(\"credit_score\") >= 650, \"Standard\")\n",
    "    .when(col(\"credit_score\") >= 550, \"Subprime\")\n",
    "    .otherwise(\"High Risk\").alias(\"user_segment\")\n",
    ")\n",
    "\n",
    "# Write to MySQL\n",
    "# dim_user_df.write \\\n",
    "#     .format(\"jdbc\") \\\n",
    "#     .option(\"url\", dw_mysql_url) \\\n",
    "#     .option(\"dbtable\", \"dim_user\") \\\n",
    "#     .options(**dw_mysql_properties) \\\n",
    "#     .mode(\"append\") \\\n",
    "#     .save()\n",
    "\n",
    "dim_user_df.write.format(\"delta\").mode(\"overwrite\").save('s3a://warehouse/dim_user/')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "5f20657b",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Duplicate merchant_ids in transactions: 53611\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Unique merchants found: 211495\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Duplicates after join: 16182\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sample duplicates:\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----------+-----+\n",
      "|merchant_id|count|\n",
      "+-----------+-----+\n",
      "|      99621|    4|\n",
      "|      38311|   15|\n",
      "|      68579|    2|\n",
      "|      82529|   17|\n",
      "|      81900|    9|\n",
      "|      66010|  161|\n",
      "|       7833|    3|\n",
      "|      90461|   29|\n",
      "|      25591|    3|\n",
      "|      73470|    2|\n",
      "+-----------+-----+\n",
      "only showing top 10 rows\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "# 4. POPULATE dim_merchant\n",
    "\n",
    "# Check for duplicates in the source data\n",
    "duplicate_check = transactions_df.groupBy(\"merchant_id\").count().filter(col(\"count\") > 1)\n",
    "print(f\"Duplicate merchant_ids in transactions: {duplicate_check.count()}\")\n",
    "\n",
    "# Extract unique merchants from transactions\n",
    "dim_merchant_df = transactions_df.select(\n",
    "    col(\"merchant_id\"),\n",
    "    col(\"merchant_id\").alias(\"merchant_sk\"),\n",
    "    col(\"mcc\"),\n",
    "    col(\"merchant_city\"),\n",
    "    col(\"merchant_state\"), \n",
    "    col(\"zip\").alias(\"merchant_zip\")\n",
    ").distinct()  \n",
    "\n",
    "print(f\"Unique merchants found: {dim_merchant_df.count()}\")\n",
    "\n",
    "# Add merchant_type by joining with mcc_codes\n",
    "dim_merchant_df = dim_merchant_df.join(\n",
    "    mcc_codes_df.select(col(\"mcc\"), col(\"merchant_type\")), \n",
    "    on=\"mcc\", \n",
    "    how=\"left\"\n",
    ")\n",
    "\n",
    "# Check for duplicates after join\n",
    "duplicate_after_join = dim_merchant_df.groupBy(\"merchant_id\").count().filter(col(\"count\") > 1)\n",
    "print(f\"Duplicates after join: {duplicate_after_join.count()}\")\n",
    "\n",
    "# If duplicates exist, show them\n",
    "if duplicate_after_join.count() > 0:\n",
    "    print(\"Sample duplicates:\")\n",
    "    duplicate_after_join.show(10)\n",
    "\n",
    "# Force single partition and ensure no duplicates\n",
    "dim_merchant_df = dim_merchant_df.dropDuplicates([\"merchant_id\"]).coalesce(1)\n",
    "\n",
    "# Write to MySQL\n",
    "# dim_merchant_df.write.jdbc(\n",
    "#     dw_mysql_url, \n",
    "#     \"dim_merchant\", \n",
    "#     mode=\"append\", \n",
    "#     properties={\n",
    "#         **dw_mysql_properties,\n",
    "#         \"batchsize\": \"1000\",\n",
    "#         \"isolationLevel\": \"READ_UNCOMMITTED\"\n",
    "#     }\n",
    "# )\n",
    "\n",
    "dim_merchant_df.write.format(\"delta\").mode(\"overwrite\").save('s3a://warehouse/dim_merchant/')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "769cc8cc",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 5. POPULATE dim_card\n",
    "\n",
    "dim_card_df = cards_df.select(\n",
    "    col(\"card_id\"),\n",
    "    col(\"card_id\").alias(\"card_sk\"),  # Using natural key as surrogate for now\n",
    "    col(\"client_id\"),\n",
    "    col(\"card_brand\"),\n",
    "    col(\"card_type\"),\n",
    "    col(\"credit_limit\"),\n",
    "    col(\"acct_open_date\"),\n",
    "    col(\"has_chip\"),\n",
    "    col(\"card_on_dark_web\")\n",
    ")\n",
    "\n",
    "# Write to MySQL\n",
    "# dim_card_df.write \\\n",
    "#     .format(\"jdbc\") \\\n",
    "#     .option(\"url\", dw_mysql_url) \\\n",
    "#     .option(\"dbtable\", \"dim_card\") \\\n",
    "#     .options(**dw_mysql_properties) \\\n",
    "#     .mode(\"append\") \\\n",
    "#     .save()\n",
    "\n",
    "dim_card_df.write.format(\"delta\").mode(\"overwrite\").save('s3a://warehouse/dim_card/')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "a31d02a5",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fact table prepared: 13305915 rows\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "fact_df = transactions_df.alias(\"t\").join(\n",
    "    fraud_labels_df.alias(\"f\"), \n",
    "    col(\"t.transaction_id\") == col(\"f.transaction_id\"), \n",
    "    \"left\"\n",
    ").select(\n",
    "    col(\"t.transaction_id\"),\n",
    "    # Create date surrogate key (YYYYMMDD format)\n",
    "    date_format(col(\"t.trans_date\"), \"yyyyMMdd\").cast(\"int\").alias(\"date_sk\"),\n",
    "    col(\"t.client_id\").alias(\"user_sk\"),\n",
    "    col(\"t.card_id\").alias(\"card_sk\"), \n",
    "    col(\"t.merchant_id\").alias(\"merchant_sk\"),\n",
    "    col(\"t.mcc\").alias(\"mcc_sk\"),\n",
    "    col(\"t.amount\"),\n",
    "    col(\"t.use_chip\"),\n",
    "    col(\"t.errors\"),\n",
    "    # Handle fraud labels - use 'label' column instead of 'is_fraud'\n",
    "    when(col(\"f.label\").isNotNull(), \n",
    "         when(col(\"f.label\") == \"fraud\", 1).otherwise(0)\n",
    "    ).otherwise(0).alias(\"is_fraud\"),\n",
    "    lit(\"external_model\").alias(\"fraud_label_source\"),\n",
    "    lit(1).alias(\"transaction_count\")\n",
    ")\n",
    "\n",
    "print(f\"Fact table prepared: {fact_df.count()} rows\")\n",
    "\n",
    "# Write in batches to avoid memory issues and deadlocks\n",
    "# fact_df.coalesce(5).write \\\n",
    "#     .format(\"jdbc\") \\\n",
    "#     .option(\"url\", dw_mysql_url) \\\n",
    "#     .option(\"dbtable\", \"fact_transactions\") \\\n",
    "#     .option(\"batchsize\", \"50000\") \\\n",
    "#     .options(**dw_mysql_properties) \\\n",
    "#     .mode(\"append\") \\\n",
    "#     .save()\n",
    "\n",
    "fact_df.write.format(\"delta\").mode(\"overwrite\").save('s3a://warehouse/fact_transactions/')\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv (3.13.3)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
